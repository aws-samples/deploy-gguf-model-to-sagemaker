{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cffc803f-2c64-4f4b-8808-7c91e230bb02",
   "metadata": {},
   "source": [
    "## Installation and Configuration Guide for GGUF Model Deployment to SageMaker Real-Time Inference\n",
    "\n",
    "In this tutorial, you will learn how to integrate GGUF models with the llama.cpp server using the BYOC approach, and deploy them to SageMaker Real-Time Inference.\n",
    "\n",
    "### 1. Preparation  \n",
    "\n",
    "#### 1.1 Docker Validation\n",
    "Ensure the Notebook supports Docker commands for container building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3811ab-050e-4c1b-8fd9-b2b4cea0bf14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de12a51-0301-4149-9c86-301bddc1f334",
   "metadata": {},
   "source": [
    "#### 1.2 Initialize Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326c9d1-6853-4334-a1fa-df378e6066ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "S3_BUCKET_NAME = \"llama-model-file\" # Please change the Bucketname when you running this cell\n",
    "ECR_REPOSITORY_NAME = \"llama-gguf-ecr\"\n",
    "AWS_ACCOUNT_ID = subprocess.check_output(\"aws sts get-caller-identity --query Account --output text\", shell=True).decode().strip()\n",
    "AWS_REGION = subprocess.check_output(\"aws configure get region\", shell=True).decode().strip()\n",
    "ECR_URI=f\"{AWS_ACCOUNT_ID}.dkr.ecr.{AWS_REGION}.amazonaws.com\"\n",
    "ECR_REPOSITORY_URI=f\"{ECR_URI}/{ECR_REPOSITORY_NAME}\"\n",
    "IMAGE_TAG=\"latest\"\n",
    "MODEL_NAME=\"Meta-Llama-3-8B.Q4_K_M.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ee7bba-de34-41f1-b844-33cd62a99e81",
   "metadata": {},
   "source": [
    "##### Print and Validate Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaeac7d-aeb1-45b1-b365-97abe72ebc08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!echo $S3_BUCKET_NAME\n",
    "!echo $AWS_ACCOUNT_ID\n",
    "!echo $AWS_REGION\n",
    "!echo $ECR_URI\n",
    "!echo $ECR_REPOSITORY_URI\n",
    "!echo $IMAGE_TAG\n",
    "!echo $MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a70669-fee6-4ac8-ad66-44029ee35321",
   "metadata": {},
   "source": [
    "#### 1.3 Confirm permissions, need to add corresponding Push ECR permissions to IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b456ce-6d26-477e-9454-c5013f19fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Get SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get the role ARN attached to the Notebook instance\n",
    "role_arn = sagemaker_session.get_caller_identity_arn()\n",
    "print(f\"SageMaker Notebook Role ARN: {role_arn}\")\n",
    "role_name = role_arn.split('/')[-1]\n",
    "print(f\"\\nOpen this link to check ExecutionPolicy: https://console.aws.amazon.com/iam/home?#/roles/details/{role_name}?section=permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d814e3-9b5d-40a9-88d7-eddf89488220",
   "metadata": {},
   "source": [
    "##### Check ExecutionPolicy in IAM role, whether it includes the following ECR-related permissions. If not, please add them.\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Action\": [\n",
    "        \"ecr:CompleteLayerUpload\",\n",
    "        \"ecr:UploadLayerPart\",\n",
    "        \"ecr:InitiateLayerUpload\",\n",
    "        \"ecr:BatchCheckLayerAvailability\",\n",
    "        \"ecr:PutImage\",\n",
    "        \"ecr:BatchGetImage\"\n",
    "    ],\n",
    "    \"Resource\": \"arn:aws:ecr:*:*:*\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a1e552-a1b6-4817-8bc2-0d74fe430eab",
   "metadata": {},
   "source": [
    "#### 1.4 Download GGUF model file from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c58f8-b255-48d2-8ef4-d64d4f3ff071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%cd /home/ec2-user/SageMaker\n",
    "!mkdir /home/ec2-user/SageMaker/workspace\n",
    "!wget -O /home/ec2-user/SageMaker/workspace/$MODEL_NAME https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF/resolve/main/Meta-Llama-3-8B.Q4_K_M.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4c416-60c0-419f-b032-845c2cebd7d8",
   "metadata": {},
   "source": [
    "#### 1.5 Create S3 Bucket for Storing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1605b3d-ac3a-4542-9b87-8e3d9a81e889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 mb s3://$S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1924705-c6b9-463a-8b0e-c6475da2bb3a",
   "metadata": {},
   "source": [
    "#### 1.6 Upload Model file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb05dce-9ad1-4e32-a518-125e0ce3c985",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp ./workspace/Meta-Llama-3-8B.Q4_K_M.gguf s3://$S3_BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e568df-ffe3-4007-b0a2-70b023d4442b",
   "metadata": {},
   "source": [
    "#### 1.7 Create ECR Repository for Storing BYOC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f59752-5ced-4fd6-9e0e-dbcd3830b8ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws ecr create-repository --repository-name $ECR_REPOSITORY_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b09d09-7e03-4454-aa9c-3016aa862ab4",
   "metadata": {},
   "source": [
    "### 2. Build BYOC Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523012db-9282-4bb1-98b5-56839d16ac63",
   "metadata": {},
   "source": [
    "Build the code in the Docker image according to the following file structure. The structure is as follows:\n",
    "```structured text\n",
    "workspace\n",
    "|-- Dockerfile\n",
    "|-- main.py\n",
    "|-- requirements.txt\n",
    "|-- serve\n",
    "|-- server.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23b931-6e33-4b04-aa20-7759572988e0",
   "metadata": {},
   "source": [
    "- **Dockerfile**: Describes how to build the container, which will be based on the llama.cpp container.\n",
    "- **main.py**: A WSGI HTTP server based on the Flask framework, used to interact with llama.cpp and meet the hosting requirements of SageMaker inference nodes. It accepts POST requests to the `/invocations` and `/ping` endpoints.\n",
    "- **requirements.txt**: Python-related dependencies.\n",
    "- **serve**: The entry file for SageMaker inference nodes. It uses port 8080 to start the WSGI server of main.py and related processes, and interacts with the llama.cpp service.\n",
    "- **server.sh**: Uses port 8181 to start the llama.cpp runtime server.\n",
    "\n",
    "*References:*\n",
    "- *https://docs.aws.amazon.com/sagemaker/latest/dg/adapt-inference-container.html*\n",
    "- *https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e9102-74c9-4435-b171-382ae84d26f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Switch the working directory to workspace\n",
    "%cd /home/ec2-user/SageMaker/workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1983c-0e91-44f8-9e07-81cd422a751a",
   "metadata": {},
   "source": [
    "#### 2.1 Create Dockerfile\n",
    "The Dockerfile in the example uses a CUDA-based llama.cpp image to support inference on GPU. You can also use the llama.cpp:full image for CPU-based inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3747cc-188a-44ac-9789-dd417a58e1ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM ghcr.io/ggerganov/llama.cpp:full-cuda\n",
    "\n",
    "# Sets dumping log messages directly to stream instead of buffering\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "# Set MODELPATH environment variable\n",
    "ENV MODELPATH=/app/llm_model.bin\n",
    "\n",
    "ENV PATH=$PATH:/app\n",
    "\n",
    "ENV BUCKET=\"\"\n",
    "ENV BUCKET_KEY=\"\"\n",
    "ENV GPU_LAYERS=999\n",
    "\n",
    "# The working directory in the Docker image\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    unzip \\\n",
    "    libcurl4-openssl-dev \\\n",
    "    python3 \\\n",
    "    python3-pip \\\n",
    "    python3-dev \\\n",
    "    git \\\n",
    "    psmisc \\\n",
    "    pciutils \n",
    "\n",
    "# Copy requirements.txt and install Python dependencies\n",
    "COPY requirements.txt ./requirements.txt\n",
    "#main application file\n",
    "COPY main.py /app/\n",
    "#sagemaker endpoints expects serve file to run the application\n",
    "COPY serve /app/\n",
    "COPY server.sh /app/\n",
    "\n",
    "RUN chmod u+x serve\n",
    "RUN chmod u+x server.sh\n",
    "\n",
    "RUN pip3 install -r requirements.txt\n",
    "RUN export PATH=/app:$PATH\n",
    "\n",
    "ENTRYPOINT [\"/bin/bash\"]\n",
    "\n",
    "# Expose port for the application to run on, has to be 8080\n",
    "EXPOSE 8080"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa4794-99df-4fd0-9c37-d5f8bde60c50",
   "metadata": {},
   "source": [
    "#### 2.2 Create the main.py file\n",
    "\n",
    "This code is modified based on https://github.com/Mozilla-Ocho/llamafile/blob/main/llama.cpp/server/api_like_OAI.py\n",
    "\n",
    "This is a simple HTTP API server and a basic web frontend for interacting with llama.cpp. To enable GGUF model inference deployment in the Amazon SageMaker environment, this article has made targeted modifications to the original GitHub code. These modifications mainly include the following aspects:\n",
    "\n",
    "Port Configuration\n",
    "\n",
    "- The original `api_like_OAI.py` used default port 8081, connecting to llama.cpp on port 8080\n",
    "- To accommodate SageMaker inference listening ports, main.py's default port was changed to 8080, connecting to the llama.cpp API port on 8081\n",
    "\n",
    "Routing Differences\n",
    "\n",
    "- main.py modified the route from `/chat/completions` to `/invocations` to meet SageMaker inference requirements\n",
    "- A new health check route `/ping` was added in main.py\n",
    "\n",
    "Model Loading\n",
    "\n",
    "- main.py added an update_model function, integrating AWS S3 functionality to support automatic model file download when the container starts\n",
    "- Supports dynamic replacement of model files during container runtime, improving flexibility and efficiency of model management\n",
    "- After the model download is complete, a subprocess is launched on port 8181 to run the llama.cpp service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365011ac-9b3d-45c9-972e-52e4de751d51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "from asgiref.wsgi import WsgiToAsgi\n",
    "from flask import Flask, jsonify, request, Response\n",
    "import urllib.parse\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "import subprocess\n",
    "import traceback\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "slot_id = -1\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"An example of using server.cpp with a similar API to OAI. It must be used together with server.cpp.\")\n",
    "parser.add_argument(\"--chat-prompt\", type=str, help=\"the top prompt in chat completions(default: 'A chat between a curious user and an artificial intelligence assistant. The assistant follows the given rules no matter what.\\\\n')\", default='A chat between a curious user and an artificial intelligence assistant. The assistant follows the given rules no matter what.\\\\n')\n",
    "parser.add_argument(\"--user-name\", type=str, help=\"USER name in chat completions(default: '\\\\nUSER: ')\", default=\"\\\\nUSER: \")\n",
    "parser.add_argument(\"--ai-name\", type=str, help=\"ASSISTANT name in chat completions(default: '\\\\nASSISTANT: ')\", default=\"\\\\nASSISTANT: \")\n",
    "parser.add_argument(\"--system-name\", type=str, help=\"SYSTEM name in chat completions(default: '\\\\nASSISTANT's RULE: ')\", default=\"\\\\nASSISTANT's RULE: \")\n",
    "parser.add_argument(\"--stop\", type=str, help=\"the end of response in chat completions(default: '</s>')\", default=\"</s>\")\n",
    "parser.add_argument(\"--llama-api\", type=str, help=\"Set the address of server.cpp in llama.cpp(default: http://127.0.0.1:8081)\", default='http://127.0.0.1:8081')\n",
    "parser.add_argument(\"--api-key\", type=str, help=\"Set the api key to allow only few user(default: NULL)\", default=\"\")\n",
    "parser.add_argument(\"--host\", type=str, help=\"Set the ip address to listen.(default: 127.0.0.1)\", default='127.0.0.1')\n",
    "parser.add_argument(\"--port\", type=int, help=\"Set the port to listen.(default: 8080)\", default=8080)\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "def is_present(json, key):\n",
    "    try:\n",
    "        buf = json[key]\n",
    "    except KeyError:\n",
    "        return False\n",
    "    if json[key] == None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#convert chat to prompt\n",
    "def convert_chat(messages):\n",
    "    prompt = \"\" + args.chat_prompt.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "    system_n = args.system_name.replace(\"\\\\n\", \"\\n\")\n",
    "    user_n = args.user_name.replace(\"\\\\n\", \"\\n\")\n",
    "    ai_n = args.ai_name.replace(\"\\\\n\", \"\\n\")\n",
    "    stop = args.stop.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "\n",
    "    for line in messages:\n",
    "        if (line[\"role\"] == \"system\"):\n",
    "            prompt += f\"{system_n}{line['content']}\"\n",
    "        if (line[\"role\"] == \"user\"):\n",
    "            prompt += f\"{user_n}{line['content']}\"\n",
    "        if (line[\"role\"] == \"assistant\"):\n",
    "            prompt += f\"{ai_n}{line['content']}{stop}\"\n",
    "    prompt += ai_n.rstrip()\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def make_postData(body, chat=False, stream=False):\n",
    "    postData = {}\n",
    "    if (chat):\n",
    "        postData[\"prompt\"] = convert_chat(body[\"messages\"])\n",
    "    else:\n",
    "        postData[\"prompt\"] = body[\"prompt\"]\n",
    "    if(is_present(body, \"temperature\")): postData[\"temperature\"] = body[\"temperature\"]\n",
    "    if(is_present(body, \"top_k\")): postData[\"top_k\"] = body[\"top_k\"]\n",
    "    if(is_present(body, \"top_p\")): postData[\"top_p\"] = body[\"top_p\"]\n",
    "    if(is_present(body, \"max_tokens\")): postData[\"n_predict\"] = body[\"max_tokens\"]\n",
    "    if(is_present(body, \"presence_penalty\")): postData[\"presence_penalty\"] = body[\"presence_penalty\"]\n",
    "    if(is_present(body, \"frequency_penalty\")): postData[\"frequency_penalty\"] = body[\"frequency_penalty\"]\n",
    "    if(is_present(body, \"repeat_penalty\")): postData[\"repeat_penalty\"] = body[\"repeat_penalty\"]\n",
    "    if(is_present(body, \"mirostat\")): postData[\"mirostat\"] = body[\"mirostat\"]\n",
    "    if(is_present(body, \"mirostat_tau\")): postData[\"mirostat_tau\"] = body[\"mirostat_tau\"]\n",
    "    if(is_present(body, \"mirostat_eta\")): postData[\"mirostat_eta\"] = body[\"mirostat_eta\"]\n",
    "    if(is_present(body, \"seed\")): postData[\"seed\"] = body[\"seed\"]\n",
    "    if(is_present(body, \"logit_bias\")): postData[\"logit_bias\"] = [[int(token), body[\"logit_bias\"][token]] for token in body[\"logit_bias\"].keys()]\n",
    "    if (args.stop != \"\"):\n",
    "        postData[\"stop\"] = [args.stop]\n",
    "    else:\n",
    "        postData[\"stop\"] = []\n",
    "    if(is_present(body, \"stop\")): postData[\"stop\"] += body[\"stop\"]\n",
    "    postData[\"n_keep\"] = -1\n",
    "    postData[\"stream\"] = stream\n",
    "    postData[\"cache_prompt\"] = True\n",
    "    postData[\"slot_id\"] = slot_id\n",
    "    return postData\n",
    "\n",
    "def make_resData(data, chat=False, promptToken=[]):\n",
    "    resData = {\n",
    "        \"id\": \"chatcmpl\" if (chat) else \"cmpl\",\n",
    "        \"object\": \"chat.completion\" if (chat) else \"text_completion\",\n",
    "        \"created\": int(time.time()),\n",
    "        \"truncated\": data[\"truncated\"],\n",
    "        \"model\": \"LLaMA_CPP\",\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": data[\"tokens_evaluated\"],\n",
    "            \"completion_tokens\": data[\"tokens_predicted\"],\n",
    "            \"total_tokens\": data[\"tokens_evaluated\"] + data[\"tokens_predicted\"]\n",
    "        }\n",
    "    }\n",
    "    if (len(promptToken) != 0):\n",
    "        resData[\"promptToken\"] = promptToken\n",
    "    if (chat):\n",
    "        #only one choice is supported\n",
    "        resData[\"choices\"] = [{\n",
    "            \"index\": 0,\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": data[\"content\"],\n",
    "            },\n",
    "            \"finish_reason\": \"stop\" if (data.get(\"stopped_eos\", False) or data.get(\"stopped_word\", False)) else \"length\"\n",
    "        }]\n",
    "    else:\n",
    "        #only one choice is supported\n",
    "        resData[\"choices\"] = [{\n",
    "            \"text\": data[\"content\"],\n",
    "            \"index\": 0,\n",
    "            \"logprobs\": None,\n",
    "            \"finish_reason\": \"stop\" if (data.get(\"stopped_eos\", False) or data.get(\"stopped_word\", False)) else \"length\"\n",
    "        }]\n",
    "    return resData\n",
    "\n",
    "def make_resData_stream(data, chat=False, time_now = 0, start=False):\n",
    "    resData = {\n",
    "        \"id\": \"chatcmpl\" if (chat) else \"cmpl\",\n",
    "        \"object\": \"chat.completion.chunk\" if (chat) else \"text_completion.chunk\",\n",
    "        \"created\": time_now,\n",
    "        \"model\": \"LLaMA_CPP\",\n",
    "        \"choices\": [\n",
    "            {\n",
    "                \"finish_reason\": None,\n",
    "                \"index\": 0\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    if (chat):\n",
    "        if (start):\n",
    "            resData[\"choices\"][0][\"delta\"] =  {\n",
    "                \"role\": \"assistant\"\n",
    "            }\n",
    "        else:\n",
    "            resData[\"choices\"][0][\"delta\"] =  {\n",
    "                \"content\": data[\"content\"]\n",
    "            }\n",
    "            if (data[\"stop\"]):\n",
    "                resData[\"choices\"][0][\"finish_reason\"] = \"stop\" if (data.get(\"stopped_eos\", False) or data.get(\"stopped_word\", False)) else \"length\"\n",
    "    else:\n",
    "        resData[\"choices\"][0][\"text\"] = data[\"content\"]\n",
    "        if (data[\"stop\"]):\n",
    "            resData[\"choices\"][0][\"finish_reason\"] = \"stop\" if (data.get(\"stopped_eos\", False) or data.get(\"stopped_word\", False)) else \"length\"\n",
    "\n",
    "    return resData\n",
    "\n",
    "def update_model(bucket, key):\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.download_file(bucket, key, os.environ.get('MODELPATH'))\n",
    "        subprocess.run([\"/app/server.sh\", os.environ.get('MODELPATH')])\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(str(traceback.format_exc()))\n",
    "        return False\n",
    "\n",
    "@app.route('/ping', methods=['GET'])\n",
    "def ping():\n",
    "    return Response(status=200)\n",
    "\n",
    "@app.route(\"/invocations\", methods=['POST'])\n",
    "def completion():\n",
    "    if (args.api_key != \"\" and request.headers[\"Authorization\"].split()[1] != args.api_key):\n",
    "        return Response(status=403)\n",
    "    body = request.get_json()\n",
    "    stream = False\n",
    "    tokenize = False\n",
    "    if (is_present(body, \"configure\")): \n",
    "        res = update_model(body[\"configure\"][\"bucket\"], body[\"configure\"][\"key\"])\n",
    "        return Response(status=200) if (res) else Response(status=500)\n",
    "    if(is_present(body, \"stream\")): stream = body[\"stream\"]\n",
    "    if(is_present(body, \"tokenize\")): tokenize = body[\"tokenize\"]\n",
    "    postData = make_postData(body, chat=False, stream=stream)\n",
    "\n",
    "    promptToken = []\n",
    "    if (tokenize):\n",
    "        tokenData = requests.request(\"POST\", urllib.parse.urljoin(args.llama_api, \"/tokenize\"), data=json.dumps({\"content\": postData[\"prompt\"]})).json()\n",
    "        promptToken = tokenData[\"tokens\"]\n",
    "\n",
    "    if (not stream):\n",
    "        data = requests.request(\"POST\", urllib.parse.urljoin(args.llama_api, \"/completion\"), data=json.dumps(postData))\n",
    "        print(data.json())\n",
    "        resData = make_resData(data.json(), chat=False, promptToken=promptToken)\n",
    "        return jsonify(resData)\n",
    "    else:\n",
    "        def generate():\n",
    "            data = requests.request(\"POST\", urllib.parse.urljoin(args.llama_api, \"/completion\"), data=json.dumps(postData), stream=True)\n",
    "            time_now = int(time.time())\n",
    "            for line in data.iter_lines():\n",
    "                if line:\n",
    "                    decoded_line = line.decode('utf-8')\n",
    "                    resData = make_resData_stream(json.loads(decoded_line[6:]), chat=False, time_now=time_now)\n",
    "                    yield 'data: {}\\n'.format(json.dumps(resData))\n",
    "        return Response(generate(), mimetype='text/event-stream')\n",
    "\n",
    "update_model(os.environ.get('BUCKET'), os.environ.get('BUCKET_KEY'))\n",
    "\n",
    "asgi_app = WsgiToAsgi(app)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    app.run(args.host, port=args.port)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20749c-c1c2-4ba7-a7d9-7b7225b33e36",
   "metadata": {},
   "source": [
    "#### 2.3 Create requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7b4e1-9b5a-4c61-a8b3-8721bcced124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "flask\n",
    "asgiref\n",
    "boto3\n",
    "starlette\n",
    "uvicorn\n",
    "requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88207e9-733e-4b1f-bc1e-f7ffd3242df4",
   "metadata": {},
   "source": [
    "#### 2.4 Create the entry file 'serve' for SageMaker inference node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b375a0-8580-42e9-8d3d-1872460df5eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile serve\n",
    "#!/bin/sh\n",
    "echo \"serve\"\n",
    "uvicorn 'main:asgi_app' --host 0.0.0.0 --port 8080 --workers 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6b018-5058-416a-afd2-e232bbf19fb0",
   "metadata": {},
   "source": [
    "#### 2.5 Create the startup script for llama.cpp service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c04b655-9208-4829-980a-e71db0961284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile server.sh\n",
    "#!/bin/sh\n",
    "echo \"server.sh\"\n",
    "echo \"args: $1\"\n",
    "echo \"GPU Layer: $GPU_LAYERS\"\n",
    "\n",
    "# Check if NVIDIA GPU is available\n",
    "if lspci | grep -i nvidia &> /dev/null; then\n",
    "  echo \"NVIDIA GPU is available.\"\n",
    "  NGL=\"$GPU_LAYERS\"\n",
    "  CPU_PER_SLOT=1\n",
    "else\n",
    "  echo \"No NVIDIA GPU found.\"\n",
    "  NGL=0\n",
    "  CPU_PER_SLOT=4\n",
    "fi\n",
    "\n",
    "killall llama-server\n",
    "/app/llama-server -m \"$1\" -c 2048 -t $(nproc --all) --host 0.0.0.0 --port 8081 -cb -np $(($(nproc --all) / $CPU_PER_SLOT)) -ngl $NGL &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc77f00-17fb-47f6-81b0-a6622bce4300",
   "metadata": {},
   "source": [
    "#### 2.6 Package Docker Image and push to ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a968c70a-2130-4ed7-a38f-1360d36cb0b7",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a98e6-2197-460a-87dc-4c047fdba3c3",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker build -t \"$ECR_REPOSITORY_URI\":\"$IMAGE_TAG\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e0087-0cf7-40bc-885d-dbcd7e848dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push \"$ECR_REPOSITORY_URI\":\"$IMAGE_TAG\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906ba29-9a1e-4849-a20c-bdeb2035c9cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### 3. Deploy to SageMaker inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970fc318-95ea-4bd2-85b0-472f6cd2ded0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Setup role and sagemaker session\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session._region_name\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eefe35d-f3ba-4235-bc57-aa9c72d9a701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "container_uri = f\"{ECR_REPOSITORY_URI}:{IMAGE_TAG}\"\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"llama-cpp-gguf-byoc\")\n",
    "print(container_uri)\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4129c7c1-36cc-48f6-b8da-e7a55e539176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = sagemaker.Model(\n",
    "    image_uri=container_uri,\n",
    "    role=iam_role,\n",
    "    name=endpoint_name,\n",
    "    env={\n",
    "        \"MODELPATH\": f\"/app/{MODEL_NAME}\",\n",
    "        \"BUCKET\": S3_BUCKET_NAME,\n",
    "        \"BUCKET_KEY\": MODEL_NAME,\n",
    "        \"GPU_LAYERS\": \"32\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd4c942-e7b8-44ac-addd-c617902556d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Deploy your model to a SageMaker Endpoint and create a Predictor to make inference requests\n",
    "# Estimated Deploy time: 10min\"\n",
    "from datetime import datetime\n",
    "\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "model.deploy(\n",
    "    instance_type=instance_type,\n",
    "    initial_instance_count=1,\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n",
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb4fa2-e1c5-436f-98b1-ca0505c55f6b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "You can pass the S3 bucket name and object name to the SageMaker inference instance, allowing you to replace model files at runtime.\n",
    "\n",
    "---\n",
    "```python\n",
    "payload = {\n",
    "    \"configure\": {\n",
    "        \"bucket\": S3_BUCKET_NAME,\n",
    "        \"key\": MODEL_NAME\n",
    "    }\n",
    "}\n",
    "response = sagemaker_runtime.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "print(f\"response: {response}\")\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498bc48-243f-4848-b82a-115bcb8a8bbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the Invoke function, the first trigger of the model requires some time to load\n",
    "\n",
    "def invoke_sagemaker_endpoint(endpoint_name, llama_args):\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(llama_args),\n",
    "        ContentType='application/json',\n",
    "    )\n",
    "    response_body = json.loads(response['Body'].read().decode())\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f68649-e6fc-4da2-9314-092d2cdd605a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Non-streaming inference example. \n",
    "\"\"\"\n",
    "llama_args = {\n",
    "    \"prompt\": \"What are the most popular tourist attractions in Beijing?\",\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 3,\n",
    "    \"repeat_penalty\":10,\n",
    "    \"frequency_penalty\":1.1,\n",
    "    \"top_p\": 1\n",
    "}\n",
    "inference = invoke_sagemaker_endpoint(endpoint_name, llama_args)\n",
    "print(inference['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae4453-4389-4759-a00c-0b3025d972b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Streaming processing function\n",
    "\n",
    "def invoke_sagemaker_streaming_endpoint(endpoint_name, llama_args):\n",
    "    response = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(llama_args),\n",
    "        ContentType='application/json',\n",
    "    )    \n",
    "    event_stream = response['Body']\n",
    "    print(event_stream)\n",
    "    for line in event_stream:\n",
    "        itm = line['PayloadPart']['Bytes'][6:]\n",
    "        try:\n",
    "            res = json.loads(itm, strict=False )\n",
    "            print(res[\"choices\"][0][\"text\"], end='')\n",
    "        except:\n",
    "            #non-valid json, e.g. empty token \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef00a360-b046-48ad-b1ca-3e778327a56d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Streaming inference example\n",
    "to enable streaming mode, set stream=True\n",
    "\"\"\"\n",
    "\n",
    "llama_args = {\n",
    "    \"prompt\": \"What are the most popular tourist attractions in Beijing?\",\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 3,\n",
    "    \"repeat_penalty\":10,\n",
    "    \"frequency_penalty\":1.1,\n",
    "    \"top_p\": 1,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "invoke_sagemaker_streaming_endpoint(endpoint_name, llama_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9273b51-b15d-4d2e-a749-5df17e519459",
   "metadata": {},
   "source": [
    "#### Delete Model, Endpoint, Endpoint config\n",
    "*If you need to delete previously created model and endpoint, please execute the following script*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf02b89-d099-4684-be23-727066767381",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.client('sagemaker')\n",
    "response = sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "config_name = response['EndpointConfigName']\n",
    "\n",
    "# Delete the model\n",
    "try:\n",
    "    model.delete_model()\n",
    "    print(f\"Deleted model: {model.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting model: {e}\")\n",
    "\n",
    "# Delete the endpoint\n",
    "try:\n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Deleted endpoint: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint: {e}\")\n",
    "\n",
    "# If you also need to delete the endpoint configuration\n",
    "try:\n",
    "    sagemaker_client.delete_endpoint_config(EndpointConfigName=config_name)\n",
    "    print(f\"Deleted endpoint configuration: {config_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint configuration: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
